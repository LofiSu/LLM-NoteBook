# ç¬¬ä¸€ç« ï¼šLLM åŸºç¡€æ¦‚å¿µ

> ä» NLP åŸºç¡€åˆ° Transformer æ¶æ„ï¼Œæ·±å…¥ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåŸç†

## ğŸ“š ç« èŠ‚æ¦‚è¿°

æœ¬ç« å°†å¸¦ä½ ä»é›¶å¼€å§‹ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¦‚å¿µï¼ŒåŒ…æ‹¬ï¼š

- **NLP å‘å±•å†ç¨‹**ï¼šä»ä¼ ç»Ÿæ–¹æ³•åˆ°æ·±åº¦å­¦ä¹ 
- **Transformer æ¶æ„**ï¼šç°ä»£å¤§æ¨¡å‹çš„åŸºç¡€
- **æ³¨æ„åŠ›æœºåˆ¶**ï¼šç†è§£æ¨¡å‹å¦‚ä½•"å…³æ³¨"ä¿¡æ¯
- **RNN vs Transformer**ï¼šæ¶æ„æ¼”è¿›å¯¹æ¯”

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿï¼š

- ç†è§£ NLP çš„åŸºæœ¬ä»»åŠ¡å’Œå‘å±•å†ç¨‹
- æŒæ¡ Transformer æ¶æ„çš„æ ¸å¿ƒç»„ä»¶
- æ·±å…¥ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†
- å¯¹æ¯”ä¸åŒæ¶æ„çš„ä¼˜ç¼ºç‚¹

## ğŸ“– å†…å®¹å¯¼èˆª

| ä¸»é¢˜ | å†…å®¹ | çŠ¶æ€ |
|------|------|------|
| [NLP åŸºç¡€ä¸ Transformer æ¶æ„](./llm-basics.md) | NLPå‘å±•ã€TransformeråŸç† | âœ… |
| [æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£](./attention-mechanism.md) | è‡ªæ³¨æ„åŠ›ã€å¤šå¤´æ³¨æ„åŠ› | âœ… |
| [RNN ä¸ Transformer å¯¹æ¯”](./rnn-vs-transformer.md) | æ¶æ„æ¼”è¿›ã€ä¼˜ç¼ºç‚¹åˆ†æ | âœ… |
| [GPT & BERT æ¨¡å‹è§£æ](./gpt-bert-analysis.md) | ç»å…¸æ¨¡å‹æ¶æ„è¯¦è§£ | âœ… |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®çŸ¥è¯†
- åŸºç¡€çš„æœºå™¨å­¦ä¹ æ¦‚å¿µ
- Python ç¼–ç¨‹åŸºç¡€
- çº¿æ€§ä»£æ•°åŸºç¡€

### å­¦ä¹ å»ºè®®
1. å…ˆé˜…è¯» [NLP åŸºç¡€ä¸ Transformer æ¶æ„](./llm-basics.md) äº†è§£æ•´ä½“èƒŒæ™¯
2. æ·±å…¥å­¦ä¹  [æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£](./attention-mechanism.md) æŒæ¡æ ¸å¿ƒæ¦‚å¿µ
3. å¯¹æ¯”å­¦ä¹  [RNN ä¸ Transformer å¯¹æ¯”](./rnn-vs-transformer.md) ç†è§£æ¶æ„æ¼”è¿›
4. æœ€åå­¦ä¹  [GPT & BERT æ¨¡å‹è§£æ](./gpt-bert-analysis.md) äº†è§£ç»å…¸æ¨¡å‹

## ğŸ’¡ é‡ç‚¹æ¦‚å¿µ

### Transformer æ¶æ„
```mermaid
graph TD
    A[è¾“å…¥åºåˆ—] --> B[ä½ç½®ç¼–ç ]
    B --> C[å¤šå¤´è‡ªæ³¨æ„åŠ›]
    C --> D[å‰é¦ˆç¥ç»ç½‘ç»œ]
    D --> E[æ®‹å·®è¿æ¥]
    E --> F[å±‚å½’ä¸€åŒ–]
    F --> G[è¾“å‡º]
```

### æ³¨æ„åŠ›æœºåˆ¶
- **è‡ªæ³¨æ„åŠ›**ï¼šåºåˆ—å†…éƒ¨çš„ä¿¡æ¯äº¤äº’
- **å¤šå¤´æ³¨æ„åŠ›**ï¼šä»ä¸åŒè§’åº¦ç†è§£ä¿¡æ¯
- **ä½ç½®ç¼–ç **ï¼šä¿ç•™åºåˆ—ä½ç½®ä¿¡æ¯

## ğŸ”— ç›¸å…³èµ„æº

### æ¨èé˜…è¯»
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - å›¾è§£ Transformer
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805) - BERT è®ºæ–‡

### å®è·µé¡¹ç›®
- [Transformer å®ç°æ•™ç¨‹](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)

## ğŸ“ å­¦ä¹ ç¬”è®°

åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œå»ºè®®ä½ ï¼š

1. **åŠ¨æ‰‹å®è·µ**ï¼šå°è¯•å®ç°ç®€å•çš„æ³¨æ„åŠ›æœºåˆ¶
2. **ç”»å›¾ç†è§£**ï¼šç”¨å›¾è¡¨å¸®åŠ©ç†è§£å¤æ‚æ¦‚å¿µ
3. **å¯¹æ¯”å­¦ä¹ **ï¼šç†è§£ä¸åŒæ¶æ„çš„ä¼˜ç¼ºç‚¹
4. **åŠæ—¶æ€»ç»“**ï¼šè®°å½•å­¦ä¹ å¿ƒå¾—å’Œç–‘é—®

## ğŸ¯ ä¸‹ä¸€ç« é¢„å‘Š

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨å¤§æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ï¼š
- KV-Cache æœºåˆ¶
- å‰ç¼€ç¼“å­˜ä¼˜åŒ–
- è®­ç»ƒä¼˜åŒ–æŠ€æœ¯

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹æ¢ç´¢ LLM çš„å¥‡å¦™ä¸–ç•Œï¼** ğŸš€ 