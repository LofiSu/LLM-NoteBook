一面
1. 项目
2.讲讲DeepSpeed的理解
3.你为什么用zero3？
4.还用过哪些加速框架吗？有什么区别？推导一下显存节省公式。
5.内容理解怎么做？做过训练吗？如果训练一个chart story的模型，数据你会怎么构造（蒸馏其他大的模型）
6.讲一下BLIP和BLIP2的区别，现在主流模型用什么？训练过吗
7.Q-former是什么？
8.手撕：leetcode 69 手写根号，有优化方法吗

二面
1.项目
4.embedding 你训练过吗？loss是什么？
5.手撕（hot100，leetcode3. 无重复字符最长子串，
一面：
简历项目。（八股面）
1. 如何做一个专有领域大模型（sft）
2. 专有领域数据方面有什么经验（通用和专有数据配比）
3. r1蒸馏是怎么做的，提高了哪些指标，有副作用吗（通用问答下降，acc指标）
4. 用了多少卡
5. 大模型基本结构，大模型如何避免过拟合（dropout，resnet，mha-gqa，数据多样化，正则）
6. mha进化论（mha到mqa到gqa）
7. 讲一下rlhf，grpo和ppo区别（ 这里忘了没答好，只回答了没有critical  model）
8. 玩游戏吗（老玩家，ieg经典面试题）
	
二面
	
1.项目经历
2.你们post-training是怎么做的
3.有做过rag吗（没做过），讲一下基本流程
4.什么时候可以来
5.额外出一个智力题（给几百列超长用户数据你会如何分析这些数据中包含异常行为数据，没答出来）
6.玩游戏吗（玩，手游也玩，lol，绝地，原神（笑，差点没绷住，不是故意的））
6.没手撕
	
三面
	
1.项目经历
2.大模型基本结构和长文本大模型怎么训练的（简历写了）
3.八股：python虚拟内存不够咋办，python是多线程吗？
4.cuda算子写过吗（没写过）
5.如何优化大模型推理速度（模型结构优化，节省kV cache，mtp这些）
一面：
	
1.简历
2.MHA 公式，有优化吗？GQA 是怎么实现的？
3.如何防止 LLM 训练过拟合（模型）
4.梯度消失是什么？解释一下，如何缓解
5.lora的 A 和 B 怎么初始化（高斯、0），可以换过来吗？（可以）
6.RAG 做过调参吗？调参经验（没做过，讲了基本流程）
7.Adam 和adamw
手撕： 搜索推荐系统
	
二面：
	
1.简历
2.我现在要做一个客服智能体，更拟人，你会如何做？（难）
3.怎么缓解 kv cache（几种模型结构）
4.deepspeed 三种stage，分别对哪方面优化，每张卡节省多少（总量/N）
5.混合精度正常占用多少显存（12（优化器 3 fp32）+4（参数和梯度 2 bf16）=16）
6.32B 模型调要多少张卡，max_token 有限制吗？(4 node)
7.你sft有什么问题（长文本能力下降，通用知识能力下降）
手撕：hot100 链表两两反转
一面（只有一面）
1. 简历
2. 给我讲一下 GRPO 公式（讲的很细，从 old 和当前 policy 多次生成开始）
3. GRPO 有什么缺点？（相比较 PPO 少一个 critical model，相比较 DPO 慢）
4. KL 散度公式是什么，你的理解是什么
5. KL 散度和交叉熵关系，你能推导下吗？
6. 什么时候来（5 月份），能早点吗？（不能，估计到这里就凉了）
7. 你知道飞猪是什么吗？（买票的，笑了）
8. 无手撕，后面邮件一直没消息，估计去的太晚横向被 k 掉了
一面（只有一面）
	
1. 简历
2. 你的 grpo 是怎么训练的？用了多少数据，多少张卡？
3. 蒸馏数据有什么经验？（think，answer+自己做的个长度衰减）
4. 蒸馏平均多少 s 一条数据（忘了，回答大概 16s 一次 infer）
5. lora 讲讲原理
6. 梯度消失和爆炸是什么？如何缓解
7. 你自己做过哪些梯度消失和爆炸的优化，现在有个 LLM SFT 时候 loss 异常，如何定位和解决（已经神志不清了）
8. VLLM 的 page_attention 是什么？你有自己改过吗？（讲了原理，没有自己改过）
9. 做过LLM 的模型修改没有？（做的不多，改改分类头这些）
10. 讲讲 VIT，patch 论文里写的多大的（16\*16）
11. CLIP 有自己训练过吗？（没有，记得batch_size大概是 3.2W，用了 3 亿条数据）
12. CLIP 的损失是什么？正则化讲一讲（神志不清* n，忘了说l2 正则）
13. 手撕：带 mask 的 MHA、手动实现x 的平方根（这个时候已经神志不清了，二分法没做出来）
14. 反问：项目组做什么的（答：抖音大模型）
很多人问我如何准备大模型的面试，分享下我的经验，针对两种情况：
1. 有大模型实习
2. 无大模型实习
针对无大模型实习的情况，我建议先找一段中厂实习为主，比如 zhipu、Minimax （当然有大厂进大厂）这些，相对容易进，尽量积累大模型实习经历。
针对有大模型实习的情况，复习内容为：
1. 常规八股（transformer、bert 等）
2. 最新八股（GQA 等）
3. 技术报告（一定要自己看原PDF，目前推荐 DeepSeekV3, R1, kimi1.5, Minimax-01, Qwen2.5, Qwen2.5-VL）
4. 手撕 Leetcode：Hot100
5. 手撕模型，比如 MHA 这些
首先是常规八股和最新八股，这一部分可以自己找找网上资料，整理好的，背。
然后是技术报告，目前推荐的内容有：
1. DeepSeekV3：必读
2. DeepSeekR1：必读
3. Kimi1.5：选读
4. Minimax-01：选读，据我所知最长上下文模型（外推到 4M，不过好像被 LLama4 的 10M 超过了）
5. Qwen2.5：必读
6. LLama3.1：必读
7. Qwen2.5-VL（如果简历有多模态内容）：选读
重点关注：
1. 阶段训练（预训练几段、Post-training 几段？上下文用了多少？数据配比是什么？）
2. 模型创新点（MHA 创新是什么？作用是什么？）
3. 上下文优化创新点（一般是优化显存和阶段训练）
4. 多模态优化创新点（简历有多模态内容）
5. 几个模型不同之处（比如 Qwen2 和 Qwen2.5 的不同之处）
最后是手撕，Leetcode 只刷 Hot100 够了，模型手撕建议关注（我目前会的）：
1. MHA
2. LayerNorm
3. Transformer Encoder (MHA+LayerNorm+FFN)
4. PE（绝对位置编码）
5. ROPE
6. SwiGLU
7. RmsNorm
分两种情况：
1. 没有实习经历的。
2. 有实习经历的。
	
首先跳过有 a 会大佬。
	
没有实习经历：
1. 简历内容应该是实验室横向。内容有 RAG 的就行，最好有 Training 相关的（Lora、SFT、RLHF 等），只要有 Training 相关的项目，总有抽到奖的一天。
2. 项目经历应该尽可能全面。覆盖范围应该是主要热门方向，比如智能客服、文档解析、代码生成等。这些项目的特点是有自己数据结构的特点，如果有类似项目经历更容易抽到奖（第一份实习）。
	
有实习经历：(业务岗)
1. 内容应该尽可能和热门方向相关。比如 3、4 月份投简历那你的简历应该也要有蒸馏 R1 相关的内容。
2. 实习和项目经历尽可能不要重叠。比如项目经历有了 RAG、实习经历也有 RAG，那么不要都写，选一个觉得合适的写上去就行。
3. 论文不要介绍太多，在学校一栏单列一行，在投的也行。论文经历（研究经历）在业务算法岗个人认为比较尴尬，尽量不要占简历太多空间，不要抢占项目和实习经历的空间。
4. 项目经历（实习经历）的内容应该体现专有领域数据的处理。目前大多数人的简历是写蒸馏了 GSM8K 的数据 XXX，这些都烂大街了，不能提现你对一个业务的理解，因此应该写特定领域数据的处理经验，而不是仅仅用了开源的 benchmark 或数据集。
	
总结：项目和实习经历的内容应该是：
1. 覆盖范围广。Post-training、RAG
2. 热门方向。3-4 月份的 R1 蒸馏
3. 内容避免重复。
4. 专有领域数据处理经验。
5. 尽可能 写 Training 经验。
5. 减少研究经历的描写（研究岗除外）。
	
另外，最好写一篇论文，现在投会门槛低，写了投出去后Openreview就有记录，也能写简历上在投。
开源项目经历推荐（我用过的）：
1.文档解析：MinerU
2.目标检测：Yolo
3.模型微调：Llama-factory
4.OCR：Paddle，rapid_ocr
5.NER：uie
6.记忆模块：mem0
7.向量数据库：Milvus
这些是工具类，在业务岗中用到比较多，大家可以根据目标厂选择性使用。
阿里通义
一面：讲论文项目，拷打简历； Code 螺旋矩阵 + 旋转数组二分查找。
二面：拷打细节，大概是，回复过程中，针对漏洞追问，回复方案后继续追问。很多都没get到面试官想问的点。
	
字节电商大模型
一面： 讲论文，拷打简历；大模型训练细节相关；R1的训练阶段， code螺旋矩阵。
二面：感觉不匹配主动终止。
	
腾讯wxg
一面：讲论文，拷打细节；Code 字符串中包含query字符的最短子串。
二面：讲论文，拷打简历；强化学习ppo dpo grpo区别；ppo细节，value model和reward model区别；GRPO中clip和kl散度约束作用和区别；off-policy和on-policy的优缺点；Code最长公共子序列。
	
阿里电商大模型
一面：讲论文和项目，聊RAG瓶颈和后面发展看法，Lora训练相关，code最长递增子序列+手撕MHA

